{
  "name": "Anurag Ghosh",
  "tagline": "Undergrad Researcher in Computer Vision",
  "body": "# Update on Visual Localization\r\n\r\nWe extracted video frames with associated GPS coordinates. As we had 1 gps coordinate per second and the video was recorded at 30 frames per second. We extracted 2 frames each second from our training videos and assigned them the same GPS coordinates as we know that there wouldn't be much change in GPS value in a second. This resulted in a training set of 5982 images and associated GPS coordinates. For our test set, we similarly extracted 1 frame per second from the test set which do not overlap with our training set (different set of videos) to get 915 frames to test on.\r\n\r\nWe then  trained a BagOfVisualWords model with a vocabulory size of 500,000 and extracted SIFT features from the images. Our idea of localization involved retrieving the closest training samples for each of the test samples. The top 'k' training samples were considered and their GPS coordinate were averaged to get an estimate of the GPS coordinate of the test sample. \r\n\r\n### Random Sample\r\n![Random Sample](https://raw.githubusercontent.com/anuragxel/anuragxel.github.io/master/ReportImages/VL_report_data.png)\r\n\r\n### Result of the random sample\r\n![Random Sample Result](https://raw.githubusercontent.com/anuragxel/anuragxel.github.io/master/ReportImages/VL_report_data_result.png)\r\n\r\nThe difference in the estimate and the actual coordinates was calculated using Harversine formula which is a widely used formula. Two measures were calculated for different values of 'k', first the mean error distance (mean) (in metres) and other is hit rate of samples (ht10, ht15) above a certain threshold distance (in metres). The values are tabulated below -\r\n\r\n| k      | 1  | 3  | 5  | 10 | 15 |\r\n|--------|----|----|----|----|----|\r\n|mean    |6.8 |6.3 |6.1 |5.8 |5.7 |\r\n|hit (10)|80.3|81.4|81.0|80.4|79.5|\r\n|hit (15)|90.5|93.3|95.5|97.5|97.5|\r\n\r\n***\r\n\r\n# Update on Dataset\r\n\r\nThe recorded egocentric videos are of good quality but the associated GPS quality is very bad in indoor areas. So, it was necessary to clip out parts of the video so that experiments can be performed on the embedded video.\r\n\r\n![Error in Indoor Areas](https://raw.githubusercontent.com/anuragxel/anuragxel.github.io/master/ReportImages/GPS_Data_Error.png)\r\n\r\n![Cleaned up data](https://raw.githubusercontent.com/anuragxel/anuragxel.github.io/master/ReportImages/GPS_Data_cleaned.png)\r\n\r\nAlso, a ground truth was established for edges.\r\n\r\n![Edges of the graph](https://raw.githubusercontent.com/anuragxel/anuragxel.github.io/master/ReportImages/Graph_edges.png)\r\n\r\nTextual data has been collected, categorized (into glue and content sentences) and cleaned up accordingly. Me and Yash are currently working on implementing the Bag of Visual Words for visual localization and integrating that to the pipeline, we will be finishing that and running benchmarks **by this weekend**.Also, we will be looking to smooth the gps noise using a **Kalman filter**(http://www.mdpi.com/1424-8220/13/11/15307/pdf) and decide whether such an approach is a good idea or not.\r\n\r\n### Hello!\r\nThis is the page where I'll post updates on my research work.",
  "google": "",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}